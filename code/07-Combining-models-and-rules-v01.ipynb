{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Models and Rules\n",
    "\n",
    "Combining statistical models with rule-based systems, it is one of the most powerful tricks you should have in your nlp toolbooks. In this chapter we will look at how to do it with spaCy.\n",
    "\n",
    "![](../imgs/stat-models-vs-rules-v01.png)\n",
    "\n",
    "Statistical models are useful if your application needs to be able to generalize based on a few examples. For instance, detecting product or person names usually benefits from a statistical model. Instead of providing a list of all person names ever your application will be able to predict whether a span of tokens is a person's name. Similarly, you can predict dependency labels to find subject-object relationships. To do this you will use spaCy entity recognizer, dependency parcder or part-of-speech tagger.\n",
    "\n",
    "![](../imgs/stat-models-vs-rules-v02.png)\n",
    "\n",
    "Rule-based approaches on the other hand comming handy if there is a more or less finite number of instances you want to find. For example all countries or cities of the world, drug names or even dog breeds. In spaCy you can achive this with custom tokenization rules as well as `Matcher` and `PhraseMatcher`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Rule-based Matching\n",
    "\n",
    "In the last chapter you've learn how to use spaCy rule-based matcher to find complex patterns in your text. Here it is a quick recap. \n",
    "\n",
    "The `Matcher` is initialized with the shared vocabulary, usually `nlp.vocab`.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "# Initialize with the shared vocab\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "```\n",
    "\n",
    "Patterns are lists of dictionaries describing the tokens and each dictionary describes one token and each attributes.\n",
    "\n",
    "Patters can be added to the matcher using the `matcher.add` method:\n",
    "\n",
    "```python\n",
    "# Patterns are lists of dictionaries describing the tokens\n",
    "pattern = [{'lemma': 'love','pos': 'VERB'}, {'lower': 'cats'}]\n",
    "matcher.add('LOVE_CATS', None, pattern)\n",
    "```\n",
    "\n",
    "Operators can specify how often to match a token. For example `+` will match one or more times:\n",
    "\n",
    "```python\n",
    "# Operators can specify how often a token should be matched\n",
    "pattern = [{'orth': 'very','OP': '+'}, {'orth': 'happy'}]\n",
    "```\n",
    "Calling the `matcher` on a `doc` object, will return a list of the matches. It match is a tuple consisting of id and start and end token index in the document\n",
    "\n",
    "```python\n",
    "# Calling matcher on `doc` returns list of (match_id, start, end) tuples\n",
    "doc = nlp(\"I love cats and I'm very very happy\")\n",
    "matches = matcher(doc)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding statistical predictions\n",
    "\n",
    "Here it is an example of the matcher rule for Golden Retriever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a larger model with vectos\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('DOG', None, [{'lower': 'golden'}, {'lower': 'retriever'}])\n",
    "\n",
    "doc = nlp(\"I have a Golden Retriever\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we iterate over the matches returned by matcher we can get the match id and the start and index of the matched span:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever 3 5\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text, span.start, span.end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then find more about it. Span object give us access to the original document in all other token attributes and linguistic features predicted by the model. For example, we can get the span root token.\n",
    "\n",
    "\n",
    "If the span consists of more than one token this will be the token that decides the category of the phrase. For example, the root of the Golden Retriever is Retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root token: Retriever\n"
     ]
    }
   ],
   "source": [
    "# Get the span's root token and root head token\n",
    "print('Root token:', span.root.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also can find the head token of the root. This is the sintactic pattern that governs the phrase. In this case the verb \"have\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root head token: have\n"
     ]
    }
   ],
   "source": [
    "print('Root head token:', span.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can look at the previous token and its attributes. In this case, it is a determinant, the article \"a\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous token: a DET \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the previous token and its POS tag\n",
    "print('Previous token:', doc[start - 1].text, doc[start - 1].pos_,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient phrase matching (1)\n",
    "\n",
    "The phrase matcher it is another helpful tool to find sequences of words on your data. It performs a keywork search on the document but instead of only finding strings it give you direct access to the tokens in context. \n",
    "\n",
    "- `PhraseMatcher` like regular expressions or keyword seach - but with access to the tokens!\n",
    "\n",
    "- It takes `Doc` objecs as patterns\n",
    "\n",
    "- It is also really fast. More efficient and faster than the `Matcher`\n",
    "\n",
    "This it makes very useful for matching large dictonaries an world lists on large volumes of text\n",
    "\n",
    "- Great for matching large word lists\n",
    "\n",
    "## Efficient phrase matching (2)\n",
    "\n",
    "Here it is an example. The `PhraseMatcher` can be imported from `spacy.matcher` and follows the same as API as a regular matcher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of a list of dictionaries we pass in a `Doc` object as the pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add('DOG', None, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then iterate over the matchers and a text which give us the matcher id and the start and the end of the match. This let us to create a span object for the match tokens Golde Retriever to analyze it in context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "# iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # get the matched span\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out these new techniques to combining rules with statistical models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging patterns (1)\n",
    "Why does this pattern not match the tokens \"Silicon Valley\" in the `doc`?\n",
    "```\n",
    "pattern = [{'LOWER': 'silicon'}, {'TEXT': ' '}, {'LOWER': 'valley'}]\n",
    "```\n",
    "```\n",
    "doc = nlp(\"Can Silicon Valley workers rein in big tech from within?\")\n",
    "```\n",
    "\n",
    "You can try it out in your IPython shell. The `matcher` with the added pattern and the `doc` are already created.\n",
    "\n",
    "\n",
    "\n",
    "Answer: The tokenizer doesn't create tokens for single spaces, so there's no token with the value ' ' in between.\n",
    "\n",
    "Correct! The tokenizer already takes care of splitting off whitespace and each dictionary in the pattern describes one token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging patterns (2)\n",
    "\n",
    "Both patterns in this exercise contain mistakes and won't match as expected. Can you fix them?\n",
    "\n",
    "The `nlp` and a `doc` have already been created for you. If you get stuck, try printing the tokens in the `doc` to see how the text will be split and adjust the pattern so that each dictionary represents one token.\n",
    "\n",
    "- Edit `pattern1` so that it correctly matches all case-insensitive mentions of `\"Amazon\"` plus a title-cased proper noun.\n",
    "- Edit `pattern2` so that it correctly matches all case-insensitive mentions of `\"ad-free\"`, plus the following noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the match patterns\n",
    "pattern1 = [{'LOWER': 'Amazon'}, {'IS_TITLE': True, 'POS': 'PROPN'}]\n",
    "pattern2 = [{'LOWER': 'ad-free'}, {'POS': 'NOUN'}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('PATTERN1', None, pattern1)\n",
    "matcher.add('PATTERN2', None, pattern2)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = open('../data/match_exercise.txt')\n",
    "content = myfile.read()\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the match patterns\n",
    "pattern1 = [{'LOWER': 'Amazon'}, {'IS_TITLE': True, 'POS': 'PROPN'}]\n",
    "pattern2 = [{'LOWER': 'ad-free'}, {'POS': 'NOUN'}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('PATTERN1', None, pattern1)\n",
    "matcher.add('PATTERN2', None, pattern2)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN1 Amazon Prime\n"
     ]
    }
   ],
   "source": [
    "# Create the match patterns\n",
    "pattern1 = [{'LOWER': 'amazon'}, {'IS_TITLE': True, 'POS': 'PROPN'}]\n",
    "pattern2 = [{'LOWER': 'ad'}, {'TEXT': '-'}, {'LOWER': 'free'}, {'POS': 'NOUN'}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('PATTERN1', None, pattern1)\n",
    "#matcher.add('PATTERN2', None, pattern2)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the answer should be:\n",
    "\n",
    "```\n",
    "<script.py> output:\n",
    "    PATTERN1 Amazon Prime\n",
    "    PATTERN2 ad-free viewing\n",
    "    PATTERN1 Amazon Prime\n",
    "    PATTERN2 ad-free viewing\n",
    "    PATTERN2 ad-free viewing\n",
    "    PATTERN2 ad-free viewing\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! For the token `'_'`, you can match on the attribute `TEXT`, `LOWER` or even `SHAPE`. All of those are correct. As you can see, paying close attention to the tokenization is very important when working with the token-based `Matcher`. Sometimes it's much easier to just match exact strings instead and use the `PhraseMatcher`, which we'll get to in the next exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient phrase matching\n",
    "Sometimes it's more efficient to match exact strings instead of writing patterns describing the individual tokens. This is especially true for finite categories of things – like all countries of the world.\n",
    "\n",
    "We already have a list of countries, so let's use this as the basis of our information extraction script. A list of string names is available as the variable `COUNTRIES`. The `nlp` object and a test doc have already been created and the `doc.text` has been printed to the shell.\n",
    "\n",
    "- Import the `PhraseMatcher` and initialize it with the shared vocab as the variable `matcher`.\n",
    "- Use the `nlp` object to create one phrase pattern per country in `COUNTRIES`.\n",
    "Call the matcher on the `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'COUNTRIES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-41048497b51a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Create pattern Doc objects and add them to the matcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpatterns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcountry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCOUNTRIES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'COUNTRY'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'COUNTRIES' is not defined"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "patterns = [nlp(country) for country in COUNTRIES]\n",
    "matcher.add('COUNTRY', None, *patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer should be:\n",
    "\n",
    "```\n",
    "<script.py> output:\n",
    "    [Czech Republic, Slovakia]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting countries and relationships\n",
    "In the previous exercise, you wrote a script using spaCy's `PhraseMatcher` to find country names in text. Let's use that country matcher on a longer text, analyze the syntax and update the document's entities with the matched countries. The `nlp` object has already been created.\n",
    "\n",
    "The text is available as the variable `text`, the `PhraseMatcher` with the country patterns as the variable `matcher`. The `Span` class has already been imported.\n",
    "\n",
    "- Iterate over the matches and create a `Span` with the label `\"GPE\"` (geopolitical entity).\n",
    "- Overwrite the entities in `doc.ents` and add the matched span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Germany', 'GPE'), ('U.S.A', 'GPE'), ('Spain', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "text = \"In Germany the trees are green. In U.S.A the cars are black. In Spain the\"\n",
    "# Create a doc and find matches in it\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\"\n",
    "    span = Span(doc, start, end, label='GPE')\n",
    "\n",
    "    # Overwrite the doc.ents and add the span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "    \n",
    "# Print the entities in the document\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == 'GPE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Update the script and get the matched span's root head token.\n",
    "- Print the text of the head token and the span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Germany', 'GPE'), ('U.S.A', 'GPE'), ('Spain', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "text = \"In Germany the trees are green. In U.S.A the cars are black. In Spain the\"\n",
    "# Create a doc and find matches in it\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\"\n",
    "    span = Span(doc, start, end, label='GPE')\n",
    "\n",
    "    # Overwrite the doc.ents and add the span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "    \n",
    "# Print the entities in the document\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == 'GPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In Germany the trees are green. In U.S.A the cars are black. In Spain the\"\n",
    "# Create a doc and find matches in it\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\" and overwrite the doc.ents\n",
    "    span = Span(doc, start, end, label='GPE')\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "    \n",
    "    # Get the span's root head token\n",
    "    span_root_head = span.root.head\n",
    "    # Print the text of the span root's head token and the span text\n",
    "    print(span_root_head.text, '-->', span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer:\n",
    "```\n",
    "Haiti --> Haiti\n",
    "Mozambique --> Mozambique\n",
    "Somalia --> Somalia\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! Now that you've practiced combining predictions with rule-based information extraction, you're ready for chapter 3, which will teach you everything about spaCy's processing pipelines.\n",
    "\n",
    "You have finished the chapter \"Large-scale data analysis with spaCy\"!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
