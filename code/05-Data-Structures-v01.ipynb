{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structures: Doc, Span and Token\n",
    "\n",
    "Now you know all about the vocabulary and String Store, we can take a look to the most important data structure: the `Doc` and its views `Token` and `Span`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `Doc` object\n",
    "\n",
    "The `Doc` is one of the central data structures in spaCy. It is created automatically when you processes a text with the `nlp` object. But you can also instatinaze the class manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an nlp object\n",
    "from spacy.lang.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the `nlp` object we can import the `doc` class from `spacy.tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Doc class\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we creating a `doc` free words. The spaces are lists of boolean values indicating wheather the word is followed by space. Every token includes that information, event the last one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The words and spaces to create the doc from\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `doc` class takes three arguments, the shared `vocab`, the words and the spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The span object (1)\n",
    "\n",
    "The `span` object is a slide of the `doc` consisting of one or more tokens.  \n",
    "\n",
    "![](../imgs/span-object.png)\n",
    "\n",
    "The `span` takes at least three arguments, the `doc` it refers to and the start and end index of the span. Remember that the end index is exclusive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Span object (2)\n",
    "\n",
    "To create an `span` object manually, we can also import the class from `spacy.token`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can the instanciateze with the `doc` and `span` start and end index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The words and spaces to create the doc from\n",
    "words = ['Hello', 'word', '!']\n",
    "spaces = [True, False, False]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create a doc manually\n",
    "doc = Doc[nlp.vocab, words=words, spaces=spaces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like\n"
     ]
    }
   ],
   "source": [
    "print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add an entity label to the span we can pass in the `label` name as the `label` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E084] Error assigning label ID 12946562419758953770 to span: not in StringStore.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-e226d067a730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a span with a label:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mGREETING\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mu'GREETING'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get hash value of entity label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspan_with_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGREETING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan_with_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan_with_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E084] Error assigning label ID 12946562419758953770 to span: not in StringStore."
     ]
    }
   ],
   "source": [
    "# Create a span with a label:\n",
    "GREETING = doc.vocab.strings[u'GREETING'] # get hash value of entity label\n",
    "span_with_label = Span(doc, 0, 2, label=GREETING)\n",
    "print(span_with_label.text, span_with_label.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For consistency, we usually write label names in capital letters.\n",
    "\n",
    "The `doc.ents` are writable. So, we can add entities manually by over writting with the list of the span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best practices\n",
    "\n",
    "A few tips and trikcs before we get started. \n",
    "\n",
    "- `Doc` and `Span` are very powerful and optimized for performance. They give you access to all references and relationships of the words and sentences.\n",
    "    - If you application its to output strings make sure to convert the `doc` as late as possible. If you do it too early you loose all relationships between the tokens.\n",
    "    - To keep thinks consistent try to use in-build token attributes wherever possible - for example, `token.i` for the token index\n",
    "- Also, don't forget to always pass in the shared `vocab`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a `Doc`\n",
    "\n",
    "Let's create some `Doc` objects from scratch! The `nlp` object has already been created for you.\n",
    "\n",
    "- Import the `Doc` from `spacy.tokens`.\n",
    "- Create a `Doc` from the `words` and `spaces`. Don't forget to pass in the vocab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is cool!\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"spaCy is cool!\"\n",
    "words = ['spaCy', 'is', 'cool', '!']\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the `Doc` from `spacy.tokens`.\n",
    "- Create a `Doc` from the words and spaces. Don't forget to pass in the vocab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go, get started!\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Go, get started!\"\n",
    "words = ['Go', ',', 'get', 'started', '!']\n",
    "spaces = [False, True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import the `Doc` from `spacy.tokens`.\n",
    "- Complete the `words` and `spaces` to match the desired text and create a `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, really?!\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Oh, really?!\"\n",
    "words = ['Oh', ',', 'really', '?', '!']\n",
    "spaces = [False, True, False, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs, spans and entities from scratch\n",
    "In this exercise, you'll create the `Doc` and `Span` objects manually, and update the named entities – just like spaCy does behind the scenes. A shared `nlp` object has already been created.\n",
    "\n",
    "- Import the `Doc` and `Span` classes from `spacy.tokens`.\n",
    "- Use the `Doc` class directly to create a doc from the words and spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like David Bowie\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = ['I', 'like', 'David', 'Bowie']\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a `Span` for \"David Bowie\" from the `doc` and assign it the label \"PERSON\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Bowie PERSON\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=['I', 'like', 'David', 'Bowie'], spaces=[True, True, True, False])\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "PERSON = doc.vocab.strings[u'PERSON'] # get hash value of entity label\n",
    "span = Span(doc, 2, 4, label=PERSON)\n",
    "print(span.text, span.label_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference https://spacy.io/usage/linguistic-features#setting-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-555d48244725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mspan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"PERSON\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required"
     ]
    }
   ],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=['I', 'like', 'David', 'Bowie'], spaces=[True, True, True, False])\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, span.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overwrite the `doc.ents` with a list of one entity, the \"David Bowie\" span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-3b73ec482b52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mspan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'PERSON'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Add the span to the doc's entities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required"
     ]
    }
   ],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=['I', 'like', 'David', 'Bowie'], spaces=[True, True, True, False])\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label='PERSON')\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be:\n",
    "\n",
    "[('David Bowie', 'PERSON')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Creating spaCy's objects manually and modifying the entities will come in handy later when you're writing your own information extraction pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structures best practices\n",
    "The code in this example is trying to analyze a text and collect all proper nouns. If the token following the proper noun is a verb, it should also be extracted. A `doc` object has already been created.\n",
    "\n",
    "```python\n",
    "# Get all tokens and part-of-speech tags\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check if the current token is a proper noun\n",
    "    if pos == 'PROPN':\n",
    "        # Check if the next token is a verb\n",
    "        if pos_tags[index + 1] == 'VERB':\n",
    "            print('Found a verb after a proper noun!')\n",
    "```\n",
    "\n",
    "### Question\n",
    "\n",
    "Why is the code bad?\n",
    "\n",
    "### Possible Answers\n",
    "- The tokens in the result should be converted back to Token objects. This will let you reuse them in spaCy.\n",
    "- It only uses lists of strings instead of native token attributes. This is often less efficient, and can't express complex relationships.\n",
    "- `pos_` is the wrong attribute to use for extracting proper nouns. You should use `tag_` and the 'NNP'and 'NNS' labels instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tokens and part-of-speech tags\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check if the current token is a proper noun\n",
    "    if pos == 'PROPN':\n",
    "        # Check if the next token is a verb\n",
    "        if pos_tags[index + 1] == 'VERB':\n",
    "            print('Found a verb after a proper noun!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found a verb after a proper noun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above second answer is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rewrite the code to use the native token attributes instead of a list of `pos_tags`.\n",
    "- Loop over each `token` in the `doc` and check the `token.pos_` attribute.\n",
    "- Use `doc[token.i + 1]` to check for the next token and its `.pos_ attribute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tokens and part-of-speech tags\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check if the current token is a proper noun\n",
    "    if pos == 'PROPN':\n",
    "        # Check if the next token is a verb\n",
    "        if pos_tags[index + 1] == 'VERB':\n",
    "            print('Found a verb after a proper noun!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == 'PROPN':\n",
    "        # Check if the next token is a verb\n",
    "        if doc[token.i + 1].pos_ == 'VERB':\n",
    "            print('Found a verb after a proper noun!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
